{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 5: Feature Selection and LASSO (Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (using GraphLab Create, though you can use other solvers). You will:\n",
    "* Run LASSO with different L1 penalties.\n",
    "* Choose best L1 penalty using a validation set.\n",
    "* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n",
    "\n",
    "In the second notebook, you will implement your own LASSO solver, using coordinate descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up graphlab create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales = graphlab.SFrame('./Week01/data/kc_house_data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Week 2, we consider features that are some transformations of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "sales['sqft_living_sqrt'] = sales['sqft_living'].apply(sqrt)\n",
    "sales['sqft_lot_sqrt'] = sales['sqft_lot'].apply(sqrt)\n",
    "sales['bedrooms_square'] = sales['bedrooms']*sales['bedrooms']\n",
    "\n",
    "# In the dataset, 'floors' was defined with type string, \n",
    "# so we'll convert them to float, before creating a new feature.\n",
    "sales['floors'] = sales['floors'].astype(float) \n",
    "sales['floors_square'] = sales['floors']*sales['floors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n",
    "* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn regression weights with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit a model with all the features available, plus the features we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = ['bedrooms', 'bedrooms_square',\n",
    "            'bathrooms',\n",
    "            'sqft_living', 'sqft_living_sqrt',\n",
    "            'sqft_lot', 'sqft_lot_sqrt',\n",
    "            'floors', 'floors_square',\n",
    "            'waterfront', 'view', 'condition', 'grade',\n",
    "            'sqft_above',\n",
    "            'sqft_basement',\n",
    "            'yr_built', 'yr_renovated']\n",
    "len(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying L1 penalty requires adding an extra parameter (`l1_penalty`) to the linear regression call in GraphLab Create. (Other tools may have separate implementations of LASSO.)  Note that it's important to set `l2_penalty=0` to ensure we don't introduce an additional L2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Linear regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 21613\n",
      "PROGRESS: Number of features          : 17\n",
      "PROGRESS: Number of unpacked features : 17\n",
      "PROGRESS: Number of coefficients    : 18\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000002  | 0.164482     | 6962915.603493     | 426631.749026 |\n",
      "PROGRESS: | 2         | 3        | 0.000002  | 0.178531     | 6843144.200219     | 392488.929838 |\n",
      "PROGRESS: | 3         | 4        | 0.000002  | 0.192980     | 6831900.032123     | 385340.166783 |\n",
      "PROGRESS: | 4         | 5        | 0.000002  | 0.209199     | 6847166.848958     | 384842.383767 |\n",
      "PROGRESS: | 5         | 6        | 0.000002  | 0.224484     | 6869667.895833     | 385998.458623 |\n",
      "PROGRESS: | 6         | 7        | 0.000002  | 0.237837     | 6847177.773672     | 380824.455891 |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+--------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "model_all = graphlab.linear_regression.create(sales, target='price', features=all_features,\n",
    "                                              validation_set=None, \n",
    "                                              l2_penalty=0., l1_penalty=1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find what features had non-zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': None, 'name': '(intercept)', 'value': 274873.0559504957}\n",
      "{'index': None, 'name': 'bathrooms', 'value': 8468.531086910105}\n",
      "{'index': None, 'name': 'sqft_living', 'value': 24.420720982445467}\n",
      "{'index': None, 'name': 'sqft_living_sqrt', 'value': 350.06055338605626}\n",
      "{'index': None, 'name': 'grade', 'value': 842.0680348976222}\n",
      "{'index': None, 'name': 'sqft_above', 'value': 20.024722417091112}\n"
     ]
    }
   ],
   "source": [
    "coefficents = model_all.get(\"coefficients\")\n",
    "\n",
    "features = coefficents[(coefficents['value'] > 0)]\n",
    "for f in features:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n",
    "\n",
    "***QUIZ QUESTION***:\n",
    "According to this list of weights, which of the features have been chosen? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting an L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n",
    "* Split our sales data into 2 sets: training and test\n",
    "* Further split our training data into two sets: train, validation\n",
    "\n",
    "Be *very* careful that you use seed = 1 to ensure you get the same answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(training_and_validation, testing) = sales.random_split(.9,seed=1) # initial train/test split\n",
    "(training, validation) = training_and_validation.random_split(0.5, seed=1) # split training into train and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a loop that does the following:\n",
    "* For `l1_penalty` in [10^1, 10^1.5, 10^2, 10^2.5, ..., 10^7] (to get this in Python, type `np.logspace(1, 7, num=13)`.)\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `l1_penalty=l1_penalty` and `l2_penalty=0.` in the parameter list.\n",
    "    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n",
    "* Report which `l1_penalty` produced the lowest RSS on validation data.\n",
    "\n",
    "When you call `linear_regression.create()` make sure you set `validation_set = None`.\n",
    "\n",
    "Note: you can turn off the print out of `linear_regression.create()` with `verbose = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe14867d190>,\n",
       " <matplotlib.lines.Line2D at 0x7fe14867d350>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEACAYAAABlOdt4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFQSKqVsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1486b99d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model = graphlab.linear_regression.create(training, target='price', features=['sqft_living',],\n",
    "                                              validation_set=None, \n",
    "                                               verbose=False)\n",
    "plt.plot(validation['sqft_living'],validation['price'],'.',\n",
    "        validation['sqft_living'], model.predict(validation),'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "6.25766285142e+14\n",
      "31.6227766017\n",
      "6.25766285362e+14\n",
      "100.0\n",
      "6.25766286058e+14\n",
      "316.227766017\n",
      "6.25766288257e+14\n",
      "1000.0\n",
      "6.25766295212e+14\n",
      "3162.27766017\n",
      "6.25766317206e+14\n",
      "10000.0\n",
      "6.25766386761e+14\n",
      "31622.7766017\n",
      "6.25766606749e+14\n",
      "100000.0\n",
      "6.25767302792e+14\n",
      "316227.766017\n",
      "6.25769507644e+14\n",
      "1000000.0\n",
      "6.25776517727e+14\n",
      "3162277.66017\n",
      "6.25799062845e+14\n",
      "10000000.0\n",
      "6.25883719085e+14\n",
      "Minimum RSS:  6.25766285142e+14\n",
      "Minimum l1 penalty:  10.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_RSS(model, data, outcome):\n",
    "    # First get the predictions\n",
    "    predictions = model.predict(data)\n",
    "    # Then compute the residuals/errors\n",
    "    residual =   outcome - predictions\n",
    "    # Then square and add them up\n",
    "    \n",
    "    residual_squared = residual*residual\n",
    "    RSS = residual_squared.sum()\n",
    "\n",
    "    return(RSS)\n",
    "\n",
    "\n",
    "l1_penalties = np.logspace(1,7, num=13)\n",
    "minRSS = 9e99\n",
    "min_l1 = 9e99\n",
    "min_model = graphlab.SFrame()\n",
    "for l1 in l1_penalties:\n",
    "    #switched from all_features to features\n",
    "    print l1\n",
    "    model = graphlab.linear_regression.create(training, target='price', features=all_features,\n",
    "                                              validation_set=None, \n",
    "                                              l2_penalty=0., l1_penalty=l1, verbose=False)\n",
    "\n",
    "    RSS = get_RSS(model, validation, validation['price'])\n",
    "    \n",
    "    print RSS\n",
    "    if RSS < minRSS:\n",
    "        minRSS = RSS\n",
    "        min_l1 = l1\n",
    "        min_model = model\n",
    "        \n",
    "print \"Minimum RSS: \", minRSS\n",
    "print \"Minimum l1 penalty: \", min_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** QUIZ QUESTIONS ***\n",
    "1. What was the best value for the `l1_penalty`?\n",
    "2. What is the RSS on TEST data of the model with the best `l1_penalty`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = graphlab.linear_regression.create(training, target='price', features=all_features,\n",
    "                                              validation_set=None, \n",
    "                                              l2_penalty=0., l1_penalty=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTION***\n",
    "Also, using this value of L1 penalty, how many nonzero weights do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">index</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">(intercept)</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">19216.3232275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">bedrooms</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7926.34897802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">bedrooms_square</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2427.08976479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">bathrooms</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">23803.266029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sqft_living</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">37.25989879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sqft_living_sqrt</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1085.36233759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">sqft_lot_sqrt</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">130.43998649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">floors</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">20806.7139534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">floors_square</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">12429.2231613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">waterfront</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">644686.676976</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[17 rows x 3 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tname\tstr\n",
       "\tindex\tstr\n",
       "\tvalue\tfloat\n",
       "\n",
       "Rows: 17\n",
       "\n",
       "Data:\n",
       "+------------------+-------+---------------+\n",
       "|       name       | index |     value     |\n",
       "+------------------+-------+---------------+\n",
       "|   (intercept)    |  None | 19216.3232275 |\n",
       "|     bedrooms     |  None | 7926.34897802 |\n",
       "| bedrooms_square  |  None | 2427.08976479 |\n",
       "|    bathrooms     |  None |  23803.266029 |\n",
       "|   sqft_living    |  None |  37.25989879  |\n",
       "| sqft_living_sqrt |  None | 1085.36233759 |\n",
       "|  sqft_lot_sqrt   |  None |  130.43998649 |\n",
       "|      floors      |  None | 20806.7139534 |\n",
       "|  floors_square   |  None | 12429.2231613 |\n",
       "|    waterfront    |  None | 644686.676976 |\n",
       "+------------------+-------+---------------+\n",
       "[17 rows x 3 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "coefficents =model.get(\"coefficients\")\n",
    "\n",
    "features = coefficents[(coefficents['value'] > 0)]\n",
    "print len(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit the number of nonzero weights\n",
    "\n",
    "What if we absolutely wanted to limit ourselves to, say, 7 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to implement a simple, two phase procedure to achive this goal:\n",
    "1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n",
    "2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_nonzeros = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the larger range of values to find a narrow range with the desired sparsity\n",
    "\n",
    "Let's define a wide range of possible `l1_penalty_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+08   1.27427499e+08   1.62377674e+08   2.06913808e+08\n",
      "   2.63665090e+08   3.35981829e+08   4.28133240e+08   5.45559478e+08\n",
      "   6.95192796e+08   8.85866790e+08   1.12883789e+09   1.43844989e+09\n",
      "   1.83298071e+09   2.33572147e+09   2.97635144e+09   3.79269019e+09\n",
      "   4.83293024e+09   6.15848211e+09   7.84759970e+09   1.00000000e+10]\n"
     ]
    }
   ],
   "source": [
    "l1_penalty_values = np.logspace(8, 10, num=20)\n",
    "print l1_penalty_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement a loop that search through this space of possible `l1_penalty` values:\n",
    "\n",
    "* For `l1_penalty` in `np.logspace(8, 10, num=20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `l1_penalty=l1_penalty` and `l2_penalty=0.` in the parameter list. When you call `linear_regression.create()` make sure you set `validation_set = None`\n",
    "    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n",
    "        * *Hint: `model['coefficients']['value']` gives you an SArray with the parameters you learned.  If you call the method `.nnz()` on it, you will find the number of non-zero parameters!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: 100000000.0\n",
      "non zeros 18\n",
      "l1: 127427498.57\n",
      "non zeros 18\n",
      "l1: 162377673.919\n",
      "non zeros 18\n",
      "l1: 206913808.111\n",
      "non zeros 18\n",
      "l1: 263665089.873\n",
      "non zeros 17\n",
      "l1: 335981828.628\n",
      "non zeros 17\n",
      "l1: 428133239.872\n",
      "non zeros 17\n",
      "l1: 545559478.117\n",
      "non zeros 17\n",
      "l1: 695192796.178\n",
      "non zeros 17\n",
      "l1: 885866790.41\n",
      "non zeros 16\n",
      "l1: 1128837891.68\n",
      "non zeros 15\n",
      "l1: 1438449888.29\n",
      "non zeros 15\n",
      "l1: 1832980710.83\n",
      "non zeros 13\n",
      "l1: 2335721469.09\n",
      "non zeros 12\n",
      "l1: 2976351441.63\n",
      "non zeros 10\n",
      "l1: 3792690190.73\n",
      "non zeros 6\n",
      "l1: 4832930238.57\n",
      "non zeros 5\n",
      "l1: 6158482110.66\n",
      "non zeros 3\n",
      "l1: 7847599703.51\n",
      "non zeros 1\n",
      "l1: 10000000000.0\n",
      "non zeros 1\n"
     ]
    }
   ],
   "source": [
    "l1_min = 1e56\n",
    "l1_max -1\n",
    "for l1 in l1_penalty_values:\n",
    "    model = graphlab.linear_regression.create(training, target='price', features=all_features,\n",
    "                                              validation_set=None, \n",
    "                                              l2_penalty=0., l1_penalty=l1, verbose=False)\n",
    "    #print model\n",
    "    values = model['coefficients']['value']\n",
    "    print \"l1:\", l1\n",
    "    print \"non zeros\", values.nnz()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n",
    "\n",
    "More formally, find:\n",
    "* The largest `l1_penalty` that has more non-zeros than `max_nonzero` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_min` (we will use it later)\n",
    "* The smallest `l1_penalty` that has fewer non-zeros than `max_nonzero` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_max` (we will use it later)\n",
    "\n",
    "\n",
    "*Hint: there are many ways to do this, e.g.:*\n",
    "* Programmatically within the loop above\n",
    "* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_penalty_min = 3792690190.73\n",
    "l1_penalty_max = 2976351441.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS***\n",
    "\n",
    "What values did you find for `l1_penalty_min` and`l1_penalty_max`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n",
    "\n",
    "We will now explore the narrow region of `l1_penalty` values we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_penalty_values = np.linspace(l1_penalty_min,l1_penalty_max,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `l1_penalty=l1_penalty` and `l2_penalty=0.` in the parameter list. When you call `linear_regression.create()` make sure you set `validation_set = None`\n",
    "    * Measure the RSS of the learned model on the VALIDATION set\n",
    "\n",
    "Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzero`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08186759232e+15\n",
      "1.07763277558e+15\n",
      "1.07350454959e+15\n",
      "1.06946433543e+15\n",
      "1.0657076895e+15\n",
      "1.06079953176e+15\n",
      "1.05599273534e+15\n",
      "1.05114762561e+15\n",
      "1.04693748875e+15\n",
      "1.04323723787e+15\n",
      "1.03855473594e+15\n",
      "1.03461690923e+15\n",
      "1.02824799221e+15\n",
      "1.01829878055e+15\n",
      "1.00847716702e+15\n",
      "9.98783211266e+14\n",
      "9.89328342459e+14\n",
      "9.81188367942e+14\n",
      "9.74019450085e+14\n",
      "9.66925692362e+14\n",
      "Minimum RSS:  9.66925692362e+14\n",
      "Minimum l1 penalty:  2976351441.63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_RSS(model, data, outcome):\n",
    "    # First get the predictions\n",
    "    predictions = model.predict(data)\n",
    "    # Then compute the residuals/errors\n",
    "    residual =   outcome - predictions\n",
    "    # Then square and add them up\n",
    "    \n",
    "    residual_squared = residual*residual\n",
    "    RSS = residual_squared.sum()\n",
    "\n",
    "    return(RSS)\n",
    "\n",
    "\n",
    "#l1_penalties = np.logspace(1,7, num=13)\n",
    "minRSS = 9e99\n",
    "min_l1 = 9e99\n",
    "min_model = graphlab.SFrame()\n",
    "for l1 in l1_penalty_values:\n",
    "    #switched from all_features to features\n",
    "    model = graphlab.linear_regression.create(training, target='price', features=all_features,\n",
    "                                              validation_set=None, \n",
    "                                              l2_penalty=0., l1_penalty=l1, verbose=False)\n",
    "\n",
    "    RSS = get_RSS(model, validation, validation['price'])\n",
    "    \n",
    "    print RSS\n",
    "    if RSS < minRSS:\n",
    "        minRSS = RSS\n",
    "        min_l1 = l1\n",
    "        min_model = model\n",
    "        \n",
    "print \"Minimum RSS: \", minRSS\n",
    "print \"Minimum l1 penalty: \", min_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n",
    "2. What features in this model have non-zero coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+---------------+\n",
      "|       name       | index |     value     |\n",
      "+------------------+-------+---------------+\n",
      "|   (intercept)    |  None | 196100.937806 |\n",
      "|     bedrooms     |  None | 2181.57432107 |\n",
      "| bedrooms_square  |  None |      0.0      |\n",
      "|    bathrooms     |  None | 17962.6966612 |\n",
      "|   sqft_living    |  None | 34.1424656512 |\n",
      "| sqft_living_sqrt |  None | 789.319789078 |\n",
      "|     sqft_lot     |  None |      0.0      |\n",
      "|  sqft_lot_sqrt   |  None |      0.0      |\n",
      "|      floors      |  None |  3665.9308176 |\n",
      "|  floors_square   |  None |      0.0      |\n",
      "+------------------+-------+---------------+\n",
      "[18 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "    values = min_model['coefficients']['value']\n",
    "    values.nnz()\n",
    "    print min_model['coefficients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
